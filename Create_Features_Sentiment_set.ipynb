{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#tensorflow sentiment analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Hotel_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev=df[df['Negative_Review']!='No Negative'].Negative_Review.reset_index().drop('index',1)\n",
    "pos_rev=df[df['Positive_Review']!='No Positive'].Positive_Review.reset_index().drop('index',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479792 387848\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_rev),len(neg_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev=list(neg_rev['Negative_Review'])\n",
    "pos_rev=list(pos_rev['Positive_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews \n",
    "\n",
    "#TokTok faster than word_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "\n",
    "# Stopwords, numbers and punctuation to remove\n",
    "remove_punct_and_digits = dict([(ord(punct), ' ') for punct in string.punctuation + string.digits])\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def word_cleaner(data):\n",
    "    cleaned_word = data.lower().translate(remove_punct_and_digits)\n",
    "    words = [toktok.tokenize(sent) for sent in sent_tokenize(cleaned_word)]\n",
    "    wordsFiltered = []\n",
    "    if not words:\n",
    "        pass\n",
    "    else:\n",
    "        for w in words[0]:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "\n",
    "    return wordsFiltered\n",
    "\n",
    "def rev_to_array(rev):\n",
    "    features=np.zeros(feat_size)\n",
    "    for w in word_cleaner(rev):\n",
    "        if w in lexicon:\n",
    "            index_val=lexicon.index(w)\n",
    "            features[index_val]+=1\n",
    "    return np.array([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller set\n",
    "neg_rev=neg_rev[:100000]\n",
    "pos_rev=pos_rev[:100000]\n",
    "\n",
    "# Create Lexicon to use\n",
    "all_words=[]\n",
    "\n",
    "for i in range(len(neg_rev)):\n",
    "    all_words+=word_cleaner(neg_rev[i])\n",
    "for i in range(len(pos_rev)):\n",
    "    all_words+=word_cleaner(pos_rev[i])\n",
    "\n",
    "count=Counter(all_words)\n",
    "rel_words=[]\n",
    "\n",
    "for w in count:\n",
    "    if 100<count[w]<10000: \n",
    "        rel_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_words=[]\n",
    "\n",
    "for w in count:\n",
    "    if 1000<count[w]: \n",
    "        com_words.append(w)\n",
    "len(com_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'better' in com_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOW! Create features vectors from Lexicon\n",
    "\n",
    "feat_size=len(com_words)\n",
    "lexicon=com_words\n",
    "featureset=[]\n",
    "\n",
    "for i in range(len(neg_rev)):\n",
    "    features=np.zeros(feat_size)\n",
    "    for w in word_tokenize(neg_rev[i]):\n",
    "        if w in lexicon:\n",
    "            index_val=lexicon.index(w)\n",
    "            features[index_val]+=1\n",
    "    featureset.append([features, [0,1]])\n",
    "for i in range(len(pos_rev)):\n",
    "    features=np.zeros(feat_size)\n",
    "    for w in word_tokenize(pos_rev[i]):\n",
    "        if w in lexicon:\n",
    "            index_val=lexicon.index(w)\n",
    "            features[index_val]+=1\n",
    "    featureset.append([features, [1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and create CV and train sets\n",
    "random.shuffle(featureset)\n",
    "\n",
    "test_size=0.2\n",
    "test_size=int(len(featureset)*test_size)\n",
    "\n",
    "train_x=list(np.array(featureset)[:,0][:-test_size])\n",
    "train_y=list(np.array(featureset)[:,1][:-test_size])\n",
    "\n",
    "test_x=list(np.array(featureset)[:,0][-test_size:])\n",
    "test_y=list(np.array(featureset)[:,1][-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sets to avoid re-running\n",
    "with open('Sentiment_set.pickle','wb') as f:\n",
    "    pickle.dump([train_x,train_y,test_x,test_y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "[train_x,train_y,test_x,test_y] = pickle.load( open( \"Sentiment_set.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "%matplotlib inline\n",
    "\n",
    "#Model test number 1\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(100, input_dim=len(train_x[0]),activation=tf.nn.relu))\n",
    "#model.add(keras.layers.Dense(100,activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_x[:2000]\n",
    "partial_x_train = train_x[2000:1000000]\n",
    "y_val = train_y[:2000]\n",
    "partial_y_train = train_y[2000:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "158000/158000 [==============================] - 15s 98us/step - loss: 0.3429 - val_loss: 0.3122\n",
      "Epoch 2/5\n",
      "158000/158000 [==============================] - 17s 108us/step - loss: 0.3030 - val_loss: 0.3029\n",
      "Epoch 3/5\n",
      "158000/158000 [==============================] - 14s 89us/step - loss: 0.2924 - val_loss: 0.2992\n",
      "Epoch 4/5\n",
      "158000/158000 [==============================] - 14s 89us/step - loss: 0.2843 - val_loss: 0.2983\n",
      "Epoch 5/5\n",
      "158000/158000 [==============================] - 17s 105us/step - loss: 0.2770 - val_loss: 0.2967\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(partial_x_train),\n",
    "                    np.array(partial_y_train),\n",
    "                    epochs=5,\n",
    "                    batch_size=300,\n",
    "                    validation_data=(np.array(x_val), np.array(y_val)),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00245318  0.99754685]]\n",
      "[[ 0.75243831  0.24756172]]\n",
      "[[ 0.37685296  0.62314707]]\n"
     ]
    }
   ],
   "source": [
    "# Test with a sentence of your choise\n",
    "my_rev=rev_to_array('Just a bit noisy')\n",
    "print(model.predict(my_rev))\n",
    "my_rev=rev_to_array('great place to stay')\n",
    "print(model.predict(my_rev))\n",
    "my_rev=rev_to_array('I am sure to go back again')\n",
    "print(model.predict(my_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for w in lexicon:\n",
    "    my_rev=rev_to_array(w)\n",
    "    scores.append(model.predict(my_rev)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_scores=pd.DataFrame([lexicon,scores]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>comfy</td>\n",
       "      <td>0.970558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>location</td>\n",
       "      <td>0.965154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>birthday</td>\n",
       "      <td>0.950797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>spacious</td>\n",
       "      <td>0.945313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>quiet</td>\n",
       "      <td>0.943517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>0.941095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>welcome</td>\n",
       "      <td>0.939178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>helpful</td>\n",
       "      <td>0.918985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>amazing</td>\n",
       "      <td>0.902039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>attentive</td>\n",
       "      <td>0.901904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "373      comfy  0.970558\n",
       "258   location  0.965154\n",
       "109   birthday  0.950797\n",
       "303   spacious  0.945313\n",
       "304      quiet  0.943517\n",
       "286  beautiful  0.941095\n",
       "350    welcome  0.939178\n",
       "284    helpful  0.918985\n",
       "177    amazing  0.902039\n",
       "345  attentive  0.901904"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>small</td>\n",
       "      <td>0.0214666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.0211334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>cold</td>\n",
       "      <td>0.0197974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>charged</td>\n",
       "      <td>0.0191183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>dark</td>\n",
       "      <td>0.0185563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>slow</td>\n",
       "      <td>0.0114165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>difficult</td>\n",
       "      <td>0.0106576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>0.0100351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>tiny</td>\n",
       "      <td>0.00967075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>bit</td>\n",
       "      <td>0.00943842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0           1\n",
       "12           small   0.0214666\n",
       "87           noisy   0.0211334\n",
       "202           cold   0.0197974\n",
       "295        charged   0.0191183\n",
       "213           dark   0.0185563\n",
       "282           slow   0.0114165\n",
       "61       difficult   0.0106576\n",
       "199  uncomfortable   0.0100351\n",
       "319           tiny  0.00967075\n",
       "60             bit  0.00943842"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(words_scores.sort_values(1,ascending=False).head(10))\n",
    "display(words_scores.sort_values(1,ascending=False).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
